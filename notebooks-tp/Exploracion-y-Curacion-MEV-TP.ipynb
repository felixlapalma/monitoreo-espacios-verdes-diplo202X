{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae369d0-fb59-4e77-a928-a6417de1a666",
   "metadata": {},
   "source": [
    "# Mentoria: Monitoreo de Espacios Verdes\n",
    "\n",
    "__DIPLODATOS 2022 - FAMAF UNC__\n",
    "\n",
    "_Felix Rojo Lapalma_\n",
    "\n",
    "## Exploración y Curación de Datos \n",
    "\n",
    "En esta etapa ya hemos avanzado en (basados en el [TP-Analisis-Visualizacion](https://github.com/felixlapalma/monitoreo-espacios-verdes-diplo202X/blob/main/notebooks-tp/Analisis-y-Visualizacion-MEV-TP.ipynb)):\n",
    "\n",
    "- análisis y exploración del conjunto de datos (aunque sea superficialmente)\n",
    "- set representativo: Es decir, aprendimos que podemos llegar a trabajar con un set mas reducido (de las caracteristicas - columnas del dataframe) e igualmente vamos a estar representando gran parte del conjunto.\n",
    "- inputacion de valores:   \n",
    "    - tenemos formas de inputar valores en rangos temporales que no tenemos (para una cierta zona) utilizando otros rangos temporales (aqui aprovechamos fuertemente la estacionalidad de lo que estamos monitoreando)\n",
    "    - que un outlier en un rango temporal puede ser un valor nominal en otro rango temporal.\n",
    "\n",
    "- etc\n",
    "\n",
    "En lo que sigue vamos a aplicar mucho de lo obtenido en el TP anterior para \"curar\" nuestro dataset con diferentes recetas. Por curar nos referimos a generar los procesos necesarios (trazables y reproducibles) que nos lleven del conjunto original de datos (que debe permanecer inmutable) hasta un conjunto que pueda ser consumido por distintos algorimos de aprendizaje. Las recetas pasaran de las mas simples, hasta algunas mas elaboradas. Una de las intenciones de todo proceso de curacion es salvar, en base a ciertos supuestos, la mayor cantidad de datos (puesto que la recoleccion de los mismos, etc, suele ser un proceso costoso tanto en recursos como en tiempo, entre otras cosas).\n",
    "\n",
    "### Fuentes\n",
    "\n",
    "El presente trabajo práctico está inspirado en los correspondientes de:\n",
    "\n",
    "- [jbergamasco2019](https://github.com/jbergamasco/DiploDatos2019)\n",
    "\n",
    "- [TP-Analisis-Visualizacion](https://github.com/felixlapalma/monitoreo-espacios-verdes-diplo202X/blob/main/notebooks-tp/Analisis-y-Visualizacion-MEV-TP.ipynb)\n",
    "\n",
    "Para una introducción un poco mas extensa respecto al origen de los datos y su procesamiento se pueden referir a :\n",
    "- [ab-initio-data-build](https://github.com/felixlapalma/monitoreo-espacios-verdes-diplo202X/blob/main/data/ab-initio-dataset-build.md)\n",
    "- [ab-initio-analisis-visualizacion](https://github.com/felixlapalma/monitoreo-espacios-verdes-diplo202X/blob/main/data/ab-initio-analisis-visualizacion.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc4101-84e3-49df-8920-0593bbeb7fdc",
   "metadata": {},
   "source": [
    "## Sobre el Trabajo Requerido\n",
    "\n",
    "Se propone la elaboración de un conjunto de procedimientos (\\*.py o notebooks) que nos permitan partiendo del dataset original obtener uno nuevo y \"curado\".\n",
    "Respecto a las recetas mencionadas, son las que van a dar origen a los distintos procedimientos y nos van a permitir inferir (ya en el proximo TP) la importancia del proceso de curación.\n",
    "\n",
    "### Sobre las recetas\n",
    "\n",
    "La idea de construccion de una receta es que pueda ser aprovechada cuando le llegue un set nuevo de datos. Tanto  para su transformacion para incorporarlos al set de entramiento como para su posible uso en inferencia.\n",
    "\n",
    "__DISCLAIMER__: En lo siguiente vamos a presentar un numero limitado (~4) de recetas (para el problema en cuestión).\n",
    "\n",
    "### ab-initio\n",
    "\n",
    "- elija (basados en lo aprendido en el TP anterior) un conjunto de bandas e indices que considere representativo. Adicione a su elección:\n",
    "    - raster: raster del cual se extraen los estadísticos\n",
    "    - clouds_on_raster: nubosidad del raster completo\n",
    "    - osm_id: identificador asociado a la capa original (atlas-espacios-verdes)\n",
    "    - area_m2: area en metros cuadrados\n",
    "    - date: fecha de la muestra.\n",
    "    \n",
    "    estas variables las sumamos a los efectos practicos de representacion y filtrado.\n",
    "\n",
    "\n",
    "\n",
    "### [A] Esfuerzo Nulo\n",
    "\n",
    "- Leer los datos\n",
    "- Extraer las columnas de interes (## ab-initio)\n",
    "- Desechar valores nulos\n",
    "- Filtrar por nubosidad < 10%\n",
    "- Remocion de Outliers (a nivel general sin considerar estacionalidad)\n",
    "- Grabar los datos transformados\n",
    "\n",
    "### [B] Imputacion General + Filtrado Por Nubosidad \n",
    "\n",
    "- Leer los datos\n",
    "- Extraer las columnas de interes (## ab-initio)\n",
    "- Remoción de Outliers (a nivel general sin considerar estacionalidad)\n",
    "- Imputar valores nulos (definir el criterio, o estadistico, _media_, _mediana_, etc)\n",
    "- Filtrar por nubosidad < 10%\n",
    "- Grabar los datos transformados\n",
    "\n",
    "\n",
    "### [C] Imputacion Estacional + Filtrado Por Nubosidad \n",
    "\n",
    "- Leer los datos\n",
    "- Extraer las columnas de interes (## ab-initio)\n",
    "- Remoción de Outliers (a nivel estacional - SIN refinar por año)\n",
    "- Imputar valores nulos (a nivel estacional - definir el criterio, o estadistico, _media_, _mediana_, etc)\n",
    "- Filtrar por nubosidad:\n",
    "    - < 20%\n",
    "    - < 40%\n",
    "- Grabar los datos transformados\n",
    "\n",
    "\n",
    "### [D] Imputacion Estacional + Filtrado Por Nubosidad + Normalizacion \n",
    "\n",
    "__NOTA__: Para ciertos algoritmos puede ser necesario normalizar los datos (para facilitar su convergencia). Se le ocurre alguna forma de hacerlo? (Existen muchas alternativas, elija alguna de ellas - justificando su eleccion). Utilizando alguna que le parezca interesante, actualice la siguiente receta basada en [C]\n",
    "\n",
    "- Leer los datos\n",
    "- Extraer las columnas de interes (## ab-initio)\n",
    "- Remoción de Outliers (a nivel estacional - SIN refinar por año)\n",
    "- Imputar valores nulos (definir el criterio, o estadistico, _media_, _mediana_, etc)\n",
    "- Filtrar por nubosidad:\n",
    "    - < 40%\n",
    "- __NORMALIZACION__\n",
    "- Grabar los datos transformados\n",
    "\n",
    "\n",
    "## Sugerencia\n",
    "\n",
    "Considere cada paso de la receta como una función. Es decir al finalizar una receta tendra tantas funciones como pasos. Esto le va a permitir reordenar y reutilizar las mismas sin necesidad de reescribir ningun codigo extra. Esto le sera particularmente util para una de las preguntas de la siguiente sección.\n",
    "\n",
    "\n",
    "## Recetas + Comparativa\n",
    "\n",
    "Una vez generadas las recetas se debera generar una suerte de tabla comparativa entre las mismas, poniendo de relevancia la cantidad de datos desechados/recuperados en cada instancia. _Note que todos las recetas tienen un mismo eje_.\n",
    "\n",
    "¿Cual es su percepcion de como resultaran cada una de las recetas en un algoritmo de aprendizaje?\n",
    "\n",
    "¿Que pasa si en la receta [C] o [D] primero filtra por nubosidad y despues remueve outliers? Considere esto como variante y definalo como [C-1] y [C-2] ([D-1]/[D-2]).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## __Extra__\n",
    "\n",
    "Generar alguna receta alternativa, basada en las propuestas mencionadas (o alguna completamente diferente :-) ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a543c19-bc55-4bc3-a241-2a2395cae3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    # running on colab\n",
    "    # from link @ https://github.com/felixlapalma/monitoreo-espacios-verdes-diplo202X/blob/main/data/ab-initio-dataset-build.md\n",
    "    !gdown https://drive.google.com/file/d/1tgbIQaEXzIghcFYyd2YM9iMho4TDHHFd/view?usp=sharing --fuzzy\n",
    "    # Notar el cambio del path y el tipo de archivo (no lo descomprimimos)\n",
    "    df_=pd.read_csv('/content/espacios-verdes-indexs-cba-20170101-20220420.zip',index_col=0)\n",
    "else:\n",
    "    # recuerde descargar el dataset desde los links mencionados en el README\n",
    "    df_=pd.read_csv('../data/cba/espacios-verdes-indexs-cba-20170101-20220420.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc292e-2922-4ce9-8870-d838f46f9f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
